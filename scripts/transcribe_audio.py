#!/usr/bin/env python3
"""
æ— å­—å¹•æ—¶ä»è§†é¢‘/éŸ³é¢‘ç”Ÿæˆ VTT å­—å¹•ï¼ˆfaster-whisper æœ¬åœ°æˆ–è¿œç¨‹ APIï¼‰ã€‚
æ ¹æ® .env ä¸­ WHISPER_* é…ç½®æ‰§è¡Œï¼Œé»˜è®¤æœ¬åœ° faster-whisper-smallã€‚
"""

import os
import sys
import json
import tempfile
import subprocess
from pathlib import Path

SCRIPT_DIR = Path(__file__).resolve().parent
SKILL_ROOT = SCRIPT_DIR.parent


def _load_env():
    """ä» skill æ ¹ç›®å½•åŠ è½½ .env"""
    try:
        from dotenv import load_dotenv
        load_dotenv(SKILL_ROOT / ".env")
    except ImportError:
        pass


def _get_whisper_config():
    _load_env()
    enabled = os.environ.get("WHISPER_ENABLED", "true").strip().lower() in ("1", "true", "yes")
    mode = (os.environ.get("WHISPER_MODE") or "local").strip().lower()
    model = (os.environ.get("WHISPER_MODEL") or "small").strip().lower()
    device = (os.environ.get("WHISPER_DEVICE") or "cpu").strip().lower()
    api_url = (os.environ.get("WHISPER_API_URL") or "").strip()
    return {"enabled": enabled, "mode": mode, "model": model, "device": device, "api_url": api_url}


def _extract_audio(video_path: Path, out_audio_path: Path) -> None:
    """ç”¨ FFmpeg ä»è§†é¢‘æå– 16kHz å•å£°é“ WAVï¼Œä¾› Whisper ä½¿ç”¨ã€‚"""
    ffmpeg = (os.environ.get("FFMPEG_PATH") or "ffmpeg").strip() or "ffmpeg"
    cmd = [
        ffmpeg, "-y", "-i", str(video_path),
        "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
        str(out_audio_path)
    ]
    subprocess.run(cmd, check=True, capture_output=True)


def _segments_to_vtt(segments) -> str:
    """å°† segmentsï¼ˆæ¯é¡¹å« start, end, textï¼‰è½¬ä¸º VTT æ–‡æœ¬ã€‚"""
    from utils import seconds_to_time
    lines = ["WEBVTT", ""]
    for s in segments:
        start = s.get("start", 0)
        end = s.get("end", 0)
        text = (s.get("text") or "").strip()
        if not text:
            continue
        start_str = seconds_to_time(start, include_hours=True, use_comma=False)
        end_str = seconds_to_time(end, include_hours=True, use_comma=False)
        lines.append(f"{start_str} --> {end_str}")
        lines.append(text)
        lines.append("")
    return "\n".join(lines).strip() + "\n"


def _transcribe_local(audio_path: Path, model: str, device: str):
    """æœ¬åœ° faster-whisper è½¬å†™ï¼Œè¿”å› segment åˆ—è¡¨ã€‚"""
    from faster_whisper import WhisperModel
    print(f"   åŠ è½½æ¨¡å‹: {model} (device={device})...")
    model_instance = WhisperModel(model, device=device, compute_type="int8" if device == "cpu" else "float16")
    print(f"   è½¬å†™ä¸­...")
    segments_iter, _ = model_instance.transcribe(str(audio_path), language=None, vad_filter=True)
    segments = [{"start": s.start, "end": s.end, "text": s.text} for s in segments_iter]
    return segments


def _parse_srt_to_segments(body: str):
    """å°† SRT æ–‡æœ¬è§£æä¸º [{"start","end","text"}]ã€‚SRT æ—¶é—´æˆ³ä¸ºé€—å·åˆ†éš”æ¯«ç§’ï¼Œéœ€å…ˆæ›¿æ¢ä¸ºç‚¹ã€‚"""
    import re
    from utils import time_to_seconds
    segments = []
    blocks = body.strip().split("\n\n")
    for block in blocks:
        lines = [ln.strip() for ln in block.strip().split("\n") if ln.strip()]
        if len(lines) < 2:
            continue
        # ç¬¬ä¸€è¡Œå¯èƒ½ä¸ºåºå·ï¼Œç¬¬äºŒè¡Œä¸ºæ—¶é—´è½´
        time_line = None
        text_lines = []
        for line in lines:
            if "-->" in line:
                time_line = line
            else:
                text_lines.append(line)
        if not time_line or not text_lines:
            continue
        parts = time_line.split("-->")
        if len(parts) != 2:
            continue
        start_str = parts[0].strip().replace(",", ".")
        end_str = parts[1].strip().replace(",", ".")
        try:
            start = time_to_seconds(start_str)
            end = time_to_seconds(end_str)
        except ValueError:
            continue
        text = " ".join(text_lines).strip()
        if text:
            segments.append({"start": start, "end": end, "text": text})
    return segments


def _transcribe_remote(audio_path: Path, api_url: str):
    """è¿œç¨‹ API è½¬å†™ï¼šPOST éŸ³é¢‘ï¼ˆmultipart audio_fileï¼‰ï¼Œå“åº”ä¸º VTT/SRT æ–‡æœ¬æˆ– JSON segmentsã€‚"""
    import urllib.request
    import uuid

    # ä½¿ç”¨ multipart/form-dataï¼Œå…¼å®¹ faster-whisper ASR ç­‰ APIï¼ˆè§ references/whisper-api.mdï¼‰
    boundary_str = uuid.uuid4().hex
    boundary = boundary_str.encode()
    with open(audio_path, "rb") as f:
        audio_data = f.read()
    output_format = os.environ.get("WHISPER_API_OUTPUT_FORMAT", "srt").strip().lower()
    language = os.environ.get("WHISPER_API_LANGUAGE", "auto").strip()

    header1 = (
        b"--" + boundary + b"\r\n"
        b'Content-Disposition: form-data; name="audio_file"; filename="audio.wav"\r\n'
        b"Content-Type: application/octet-stream\r\n\r\n"
    )
    part2 = (
        b"\r\n--" + boundary + b"\r\n"
        b'Content-Disposition: form-data; name="output_format"\r\n\r\n'
        + output_format.encode() + b"\r\n"
        b"--" + boundary + b"\r\n"
        b'Content-Disposition: form-data; name="language"\r\n\r\n'
        + language.encode() + b"\r\n"
        b"--" + boundary + b"--\r\n"
    )
    data = header1 + audio_data + part2

    req = urllib.request.Request(api_url, data=data, method="POST")
    req.add_header("Content-Type", f"multipart/form-data; boundary={boundary_str}")

    try:
        with urllib.request.urlopen(req, timeout=600) as resp:
            body = resp.read().decode("utf-8", errors="replace")
    except Exception as e:
        raise RuntimeError(f"è¿œç¨‹è½¬å†™è¯·æ±‚å¤±è´¥: {e}") from e

    # è‹¥è¿”å› JSONï¼šarray [{"start","end","text"}] æˆ– object {"segments": [...]}
    try:
        raw = json.loads(body)
        if isinstance(raw, list):
            return raw
        if isinstance(raw, dict) and "segments" in raw:
            return raw["segments"]
    except json.JSONDecodeError:
        pass

    # è‹¥è¿”å›ä¸º VTT æ–‡æœ¬
    if body.strip().upper().startswith("WEBVTT"):
        import re
        from utils import time_to_seconds
        content = re.sub(r"^WEBVTT.*?\n\n", "", body, flags=re.DOTALL)
        segments = []
        for block in content.strip().split("\n\n"):
            lines = block.strip().split("\n")
            if len(lines) < 2:
                continue
            if "-->" in lines[0]:
                ts = lines[0]
                text = " ".join(lines[1:])
            else:
                continue
            parts = ts.split("-->")
            if len(parts) != 2:
                continue
            start = time_to_seconds(parts[0].strip().replace(",", "."))
            end = time_to_seconds(parts[1].strip().replace(",", "."))
            if text.strip():
                segments.append({"start": start, "end": end, "text": text.strip()})
        return segments

    # è‹¥è¿”å›ä¸º SRT æ–‡æœ¬ï¼ˆå¦‚ faster-whisper ASR æœåŠ¡ output_format=srtï¼‰
    if _looks_like_srt(body):
        return _parse_srt_to_segments(body)

    raise ValueError("è¿œç¨‹ API è¿”å›æ—¢ä¸æ˜¯ JSON segments ä¹Ÿä¸æ˜¯ VTT/SRT æ–‡æœ¬")


def _looks_like_srt(text: str) -> bool:
    """ç®€å•åˆ¤æ–­æ˜¯å¦ä¸º SRT å†…å®¹ï¼ˆåºå· + æ—¶é—´è½´ -->ï¼‰ã€‚"""
    trimmed = text.strip()
    if not trimmed or trimmed.startswith("{"):
        return False
    return "-->" in trimmed[:200]


def transcribe_audio(video_path: str, output_vtt_path: str = None) -> str:
    """
    ä»è§†é¢‘æ–‡ä»¶ç”Ÿæˆ VTT å­—å¹•ï¼ˆæ ¹æ® .env ä½¿ç”¨æœ¬åœ° faster-whisper æˆ–è¿œç¨‹ APIï¼‰ã€‚

    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_vtt_path: è¾“å‡º VTT è·¯å¾„ï¼Œé»˜è®¤ä¸º .env ä¸­ OUTPUT_DIR ä¸‹çš„ <è§†é¢‘stem>.en.vtt

    Returns:
        ç”Ÿæˆçš„ VTT æ–‡ä»¶è·¯å¾„

    Raises:
        FileNotFoundError: è§†é¢‘ä¸å­˜åœ¨
        RuntimeError: æœªå¯ç”¨ Whisper æˆ–é…ç½®é”™è¯¯
    """
    video_path = Path(video_path)
    if not video_path.exists():
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    cfg = _get_whisper_config()
    if not cfg["enabled"]:
        raise RuntimeError("æœªå¯ç”¨ Whisperï¼ˆ.env ä¸­ WHISPER_ENABLED=falseï¼‰ï¼Œæ— æ³•ä»éŸ³é¢‘ç”Ÿæˆå­—å¹•")

    if output_vtt_path is None:
        from utils import get_output_base_dir
        output_vtt_path = get_output_base_dir() / f"{video_path.stem}.en.vtt"
    else:
        output_vtt_path = Path(output_vtt_path)

    print(f"ğŸ¤ ä»éŸ³é¢‘ç”Ÿæˆå­—å¹•ï¼ˆfaster-whisperï¼Œæ¨¡å¼: {cfg['mode']}ï¼‰...")
    print(f"   è§†é¢‘: {video_path.name}")

    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    tmp.close()
    audio_path = Path(tmp.name)

    try:
        _extract_audio(video_path, audio_path)
        print(f"   å·²æå–éŸ³é¢‘: 16kHz å•å£°é“")

        if cfg["mode"] == "remote":
            if not cfg["api_url"]:
                raise RuntimeError("WHISPER_MODE=remote æ—¶éœ€åœ¨ .env ä¸­é…ç½® WHISPER_API_URL")
            segments = _transcribe_remote(audio_path, cfg["api_url"])
        else:
            segments = _transcribe_local(audio_path, cfg["model"], cfg["device"])

        if not segments:
            raise RuntimeError("è½¬å†™ç»“æœä¸ºç©º")

        vtt_content = _segments_to_vtt(segments)
        output_vtt_path.parent.mkdir(parents=True, exist_ok=True)
        output_vtt_path.write_text(vtt_content, encoding="utf-8")
        print(f"âœ… å­—å¹•å·²ç”Ÿæˆ: {output_vtt_path.name}ï¼ˆ{len(segments)} æ¡ï¼‰")
        return str(output_vtt_path)
    finally:
        if audio_path.exists():
            audio_path.unlink(missing_ok=True)


def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe_audio.py <video_path> [output.vtt]")
        print("  ä»è§†é¢‘æå–éŸ³é¢‘å¹¶ç”¨ faster-whisperï¼ˆæœ¬åœ°æˆ–è¿œç¨‹ï¼‰ç”Ÿæˆ VTT å­—å¹•ã€‚")
        print("  é…ç½®è§ .envï¼šWHISPER_MODEã€WHISPER_MODELã€WHISPER_API_URL ç­‰ã€‚")
        sys.exit(1)

    video_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None

    try:
        out = transcribe_audio(video_path, output_path)
        print(out)
    except Exception as e:
        print(f"âŒ {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
